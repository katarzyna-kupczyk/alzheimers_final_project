{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3743a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-03 16:53:32.620654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-03 16:53:32.621461: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alzheimers_final_project'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1821/1804437088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malzheimers_final_project\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alzheimers_final_project'"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from alzheimers_final_project.params import BATCH_SIZE, IMAGE_SIZE\n",
    "import tensorflow as tf\n",
    "\n",
    "#data.py\n",
    "# Insert your local path to train data and test data here:\n",
    "path_to_train_data = '/raw_data/ALzheimersDataset/train'\n",
    "path_to_test_data = '/raw_data/ALzheimersDataset/test'\n",
    "\n",
    "def train_data_loading(path_to_train_data):\n",
    "\n",
    "    # Train Generator\n",
    "    train_generator = image_dataset_from_directory(\n",
    "    path_to_train_data,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'training',\n",
    "    seed = 123)\n",
    "\n",
    "    # Validation Generator\n",
    "    validation_generator = image_dataset_from_directory(\n",
    "    path_to_train_data,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation',\n",
    "    seed = 123)\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "def test_data_loading(path_to_test_data):\n",
    "\n",
    "    # Test Generator\n",
    "    test_generator = image_dataset_from_directory(\n",
    "    path_to_test_data,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    shuffle = True,\n",
    "    seed = 123)\n",
    "\n",
    "    return test_generator\n",
    "\n",
    "#model.py\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from alzheimers_final_project.params import INPUT_SHAPE, METRICS\n",
    "\n",
    "def build_compile_model():\n",
    "\n",
    "    # DenseNet121 Base Model\n",
    "    base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Base Model + Trainable Layers\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(50, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "#params.py\n",
    "import tensorflow as tf\n",
    "\n",
    "### MODEL PARAMETERS ###\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "EPOCHS = 100\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "METRICS = ['AUC', 'accuracy', 'Recall', 'Precision']\n",
    "\n",
    "\n",
    "#preprocess.py\n",
    "\n",
    "### GCP DATA STORAGE ###\n",
    "\n",
    "BUCKET_NAME = 'alzheimers-project-699'\n",
    "BUCKET_TRAIN_DATA_PATH = 'data/AlzheimersDataset/train'\n",
    "STORAGE_LOCATION = 'models/model.joblib'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocessing(img, label):\n",
    "    img = img / 255,\n",
    "    return img, label\n",
    "\n",
    "def augment(img, label):\n",
    "    img = tf.image.random_brightness(\n",
    "        img, max_delta, seed\n",
    "    )\n",
    "    \n",
    "    img = tf.image.stateless_random_contrast(\n",
    "        img, 0.2, 0.5, seed\n",
    "    )\n",
    "        \n",
    "    return img, label\n",
    "\n",
    "##trainer.py\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from alzheimers_final_project.data import train_data_loading, test_data_loading\n",
    "from alzheimers_final_project.params import STORAGE_LOCATION, BUCKET_NAME\n",
    "from alzheimers_final_project.model import build_compile_model\n",
    "from alzheimers_final_project.preprocess import preprocessing\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.train_path = None\n",
    "        self.test_path = None\n",
    "        self.train_generator = None\n",
    "        self.validation_generator = None\n",
    "        self.test_generator = None\n",
    "\n",
    "### MODEL PIPELINE ###\n",
    "    def set_data(self):\n",
    "        parent_path = os.path.dirname(os.path.dirname(__file__))\n",
    "        train_path = os.path.join(parent_path, 'raw_data/AlzheimersDataset/train')\n",
    "        test_path = os.path.join(parent_path, 'raw_data/AlzheimersDataset/test')\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Get and preprocess data\n",
    "        self.train_generator, self.validation_generator = train_data_loading(self.train_path)\n",
    "        self.test_generator = test_data_loading(self.test_path)\n",
    "\n",
    "    def set_model(self):\n",
    "        # Autotune the process\n",
    "        AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "        self.train_generator = self.train_generator.map(preprocessing, num_parallel_calls=AUTOTUNE)\n",
    "        self.train_generator = self.train_generator.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "        self.validation_generator = self.validation_generator.map(preprocessing, num_parallel_calls=AUTOTUNE)\n",
    "        self.test_generator = self.test_generator.map(preprocessing, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "        self.train_generator = self.train_generator.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        self.validation_generator = self.validation_generator.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        self.test_generator = self.test_generator.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "        ######sel.train_generator\n",
    "\n",
    "        self.model = build_compile_model()\n",
    "        return self.model\n",
    "\n",
    "    def fit_model(self):\n",
    "        if self.model == None:\n",
    "            self.set_model()\n",
    "        es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        rop = ReduceLROnPlateau(monitor='val_loss', factor=0.005, patience=10, min_lr=0.005)\n",
    "        self.model.fit(self.train_generator, validation_data=self.validation_generator, epochs=50, callbacks=[es, rop], verbose=1)\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"evaluates the model on test set and returns the Loss, AUC, Accuracy, Recall and Precision\"\"\"\n",
    "        test_scores = self.model.evaluate(self.test_generator)\n",
    "        scores_dict = {'Loss': test_scores[0],\n",
    "                       'AUC': test_scores[1],\n",
    "                       'Accuracy': test_scores[2],\n",
    "                       'Recall': test_scores[3],\n",
    "                       'Precision': test_scores[4]}\n",
    "        return scores_dict\n",
    "\n",
    "\n",
    "### SAVE MODEL TO GCP ###\n",
    "    def save_model(self):\n",
    "        \"\"\" Save the trained model into a model.joblib file \"\"\"\n",
    "        joblib.dump(self.pipeline, 'model.joblib')\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(BUCKET_NAME)\n",
    "        blob = bucket.blob(STORAGE_LOCATION)\n",
    "        blob.upload_from_filename('model.joblib')\n",
    "        print(f\"uploaded model.joblib to gcp cloud storage under \\n => {STORAGE_LOCATION}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = Trainer()\n",
    "\n",
    "    t.set_data()\n",
    "    t.load_data()\n",
    "    t.set_model()\n",
    "    t.fit_model()\n",
    "    t.evaluate()\n",
    "\n",
    "    # Train model and save to gcp\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
